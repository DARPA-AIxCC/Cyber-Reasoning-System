import logging
import litellm
import os
import sys
import subprocess
from pathlib import Path

logging.basicConfig(level=logging.DEBUG)

# Dependencies:
# 1. Install python3.8
# 2. Install litellm package

#######
# Some configurations
#######
litellm_model = (
    (
        "oai-gpt-4o"
        if os.getenv("AIXCC_CRS_SCRATCH_SPACE", False)
        else "gpt-4o-2024-05-13"
    )
)

root_dir = Path(__file__).parent

pit_database = root_dir.joinpath("input_models")


# For prompt construction
test_harness_file = sys.argv[1]
example_pit_file = root_dir.joinpath("prompt_example", "webp.xml")

out_pit = sys.argv[2]
# For matching the pit file
subject_name = sys.argv[3]

# For checking the correctness of the pit file
initial_testcases = sys.argv[4] if len(sys.argv) > 4 else None

# The pit file generated by LLM
llm_pit_file = root_dir.joinpath("intermediary_pit.xml")
trial_times = 5


def construct_pit_generation_prompt(test_harness: str, example_pit: str) -> str:
    return (
        "Generate a Peach pit file of peach-3.0.202 for the following test harness in C.\n"
        "Please address the following requirements: \n"
        "1. Please show the relation in the size number instead of using the 'length' attribute in the data blob if any (referring to the example). Note that, there is no 'length' attribute. \n"
        "2. Please show all fixed 'value' from the test harness and set the 'token' to true. Note that, there is no 'when' attribute. \n"
        "3. Please correctly set the iterations and set the maxOccurs to 1000. \n"
        "4. Please set the Test name to 'Default' and the Data filename to '/dev/null'.\n"
        "5. In the 'Default' Test name, please refer to the example pit given. \n"
        f"```c {test_harness}```\n\n"
        "For your reference, here is an example of a Peach pit file for the Webp input.\n"
        f"```xml {example_pit}```\n"
    )


def request_llm_generation_helper(times: int) -> str:
    # Step 1: generate the inital pit file using LLM
    test_harness = open(test_harness_file, "r").read()
    example_pit = open(example_pit_file, "r").read()
    prompt = construct_pit_generation_prompt(test_harness, example_pit)

    logging.debug(prompt)

    messages = [
        {
            "role": "system",
            "content": "You are an expert programmer in the peach pit fuzzer file format.",
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]

    key = os.getenv("LITELLM_KEY", None)
    response = litellm.completion(
        model=litellm_model,
        messages=messages,
        temperature=0.8,
        top_p=1,
        base_url=os.getenv("AIXCC_LITELLM_HOSTNAME", None),
        extra_headers={"Authorization": f"Bearer {key}"} if key else {},
        custom_llm_provider="openai",
    )
    print(response)
    content = response.choices[0].message.content
    content = content.split("```xml")[1].split("```")[0].strip()
    # For debugging
    with open("init_pit.xml", "w") as f:
        f.write(content)

    # Step 2: LLM self-corrects the pit file, as the generation from the first round is slightly incorrect
    messages = [
        {
            "role": "system",
            "content": "You are an expert programmer in the peach pit fuzzer file format.",
        },
        {
            "role": "user",
            "content": prompt,
        },
        {
            "role": "assistant",
            "content": content,
        },
        {
            "role": "user",
            "content": "Do you satisfy all the requirements? Please correct and generate the Peach pit file.",
        },
    ]

    response = litellm.completion(
        model=litellm_model,
        messages=messages,
        temperature=0.8,
        top_p=1,
        base_url=os.getenv("AIXCC_LITELLM_HOSTNAME", None),
        extra_headers={"Authorization": f"Bearer {key}"} if key else {},
        custom_llm_provider="openai",
    )
    content = response.choices[0].message.content
    content = content.split("```xml")[1].split("```")[0].strip()

    with open(llm_pit_file, "w") as f:
        f.write(content)

    # cp init_kernel_pit.xml to init_kernel_pit_{times}.xml
    os.system(f"cp init_pit.xml init_pit_{times}.xml")
    os.system(f"cp {llm_pit_file} intermediary_pit_{times}.xml")
    return llm_pit_file


def check_pit_correctness(pit_file: str) -> bool:
    # check whether the pit file matches the example test cases
    # execute the command: peach -1 -inputFilePath=example_testcases -outputFilePath=dummy pit_file

    if initial_testcases is None:
        return True

    command = f"/hone/ubuntu/peach-cracker/output/linux_x86_64_release/bin/peach -1 -inputFilePath={initial_testcases} -outputFilePath=dummy {pit_file}"
    result = subprocess.run(
        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
    )
    logging.debug(f"## Execution result: {result.stdout}")
    # check if result.stdout contains "\nok\n\n"
    res = result.stdout.find("\nok\n\n")
    if res >= 0:
        logging.debug(f"## The pit file is correct.")
        return True
    else:
        logging.debug(f"## The pit file is incorrect.")
        return False


def request_llm_generation():
    for times in range(trial_times):
        pit_file = request_llm_generation_helper(times)
        if check_pit_correctness(pit_file):
            return pit_file
    return None


def llm_check_pit_correctness() -> str:
    # Based on the subject name and the test harness, we can construct a prompt
    # and ask LLM which pit file is the best match
    test_harness = open(test_harness_file, "r").read()
    # print(f"Listing {pit_database}")
    pit_list = os.listdir(pit_database)
    prompt = (f"For the subject {subject_name} with the following test harness, in the list of peach pit files {pit_list},"
              f"do you think whether there is a matched pit? If you ARE NOT CONFIDENT, please say 'None'; otherwise, return the name of the matched one surrounded in ```:\n\n"
              f"```c {test_harness}```\n NO YAPPING \n")

    logging.debug(f"Prompt to check pit correctness:\n {prompt}")
    checking_message = [
        {
            "role": "system",
            "content": "You are an expert programmer in the peach pit fuzzer file format.",
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]
    key = os.getenv("LITELLM_KEY", None)
    response = litellm.completion(
        model=litellm_model,
        messages=checking_message,
        temperature=0.8,
        top_p=1,
        base_url=os.getenv("AIXCC_LITELLM_HOSTNAME", None),
        extra_headers={"Authorization": f"Bearer {key}"} if key else {},
        custom_llm_provider="openai",
    )
    real_answer = response.choices[0].message.content
    
    if "none" in real_answer.lower():
        return None
    
    fragments = real_answer.split('```')
    
    filtered_answer = fragments[1].strip() if len(fragments) > 1 else None
    logging.debug(f"LLM answer: {real_answer}")
    logging.debug(f"LLM filtered answer: {filtered_answer}")
    
    candidate = pit_database.joinpath(filtered_answer)
    
    return candidate if candidate.exists() else None


def check_pit_from_database() -> str:
    if initial_testcases is None:
        # If no initial seeds, we delegate the task to LLM
        res = llm_check_pit_correctness()
        return res

    else:
        # traverse each pit file in the database directory
        for pit_file in os.listdir(pit_database):
            pit_file_path = os.path.join(pit_database, pit_file)
            logging.debug(f"Checking pit file: {pit_file_path}")
            if check_pit_correctness(pit_file_path):
                return pit_file_path

    return None


def main():
    # Step 1: Check the database for a matching pit file
    peach_pit_file = check_pit_from_database()

    # Step 2: If the pit file is not found, use LLM to generate the pit file
    if peach_pit_file is None:
        peach_pit_file = request_llm_generation()
        logging.debug("No matching pit file found in the database.")
    else:
        logging.debug(f"Found the matching pit file: {peach_pit_file}")

    os.system(f"cp {peach_pit_file} {out_pit}")
    return


if __name__ == "__main__":
    main()
